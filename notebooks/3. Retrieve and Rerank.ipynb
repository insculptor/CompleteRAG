{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b92a9e",
   "metadata": {},
   "source": [
    "Now that we have created the Embeddings and our Vectorstore is setup. We need to Get this knowlege in Realtime!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d2363d",
   "metadata": {},
   "source": [
    "IN this Notebook:\n",
    "- We will Take a query from User\n",
    "- Convert it into Vector Embedding\n",
    "- Search FAISS Database for matching Embeddings\n",
    "- Get Embeddings and Index of Top K Matches.\n",
    "- Out og these indexes, determine which are chunks and which ae superchunks\n",
    "    - Create a Mongo Collection: chunk -> Superchunk\n",
    "    - Get Superchunks of all fetched Embeddings\n",
    "    - Now we have a list of all superchunks fetched and all their chunks.\n",
    "    \n",
    "    - if top5 had 3 sup chunks and 2 chunks,we can have all uptill 5 super chunks and all their chunks -> 50 Chunks\\5 SuperChunks\n",
    "- Rerank the Chunks\n",
    "- Convert to  Text\n",
    "- Pass to LLM along with Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca679b",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06113661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load Environment Variables\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path('C:/Users/erdrr/OneDrive/Desktop/Scholastic/NLP/LLM/RAG/FinsightRAG/.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422716aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embeddings.models import get_gpu_memory_stats\n",
    "from src.embeddings.models import get_device\n",
    "from src.embeddings.models import get_total_gpu_memory\n",
    "from src.embeddings.models import get_embedding_model\n",
    "from src.embeddings.models import check_model_is_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9a6bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "from src.retrieval.retrieval import get_query_embeddings\n",
    "from src.retrieval.retrieval import  search_embedding\n",
    "from src.retrieval.retrieval import get_mega_chunks_by_indices\n",
    "from src.retrieval.retrieval import get_all_chunks_for_mega_chunks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbef72c",
   "metadata": {},
   "source": [
    "# Retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b64168",
   "metadata": {},
   "source": [
    "FAISS (Facebook AI Similarity Search) is a library developed by Facebook AI Research to facilitate efficient similarity search and clustering of dense vectors. It's particularly well-suited for tasks involving large-scale vector databases, like image or text embeddings. When it comes to fetching the top matching vectors for a given query embedding, FAISS provides various index types and search methods. Here's an overview of the general approach and specifically how to use HNSW (Hierarchical Navigable Small World) graphs, a method known for its efficiency in finding nearest neighbors in high-dimensional spaces.\n",
    "\n",
    "https://github.com/facebookresearch/faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daddb15",
   "metadata": {},
   "source": [
    "Faiss is built around the `Index` object. It encapsulates the set of database vectors, and optionally preprocesses them to make searching efficient. There are many types of indexes, we are going to use the simplest version that just performs brute-force L2 distance search on them: IndexFlatL2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5a0f73",
   "metadata": {},
   "source": [
    "- Explain wht we need index\n",
    "- Types of Index\n",
    "- Flat Index vs HNSW\n",
    "\n",
    "\n",
    "\n",
    "### General Approach for Fetching Top Matching Vectors\n",
    "\n",
    "1. **Choose an Index Type:** FAISS offers several index types, each optimized for different kinds of data or search requirements. The choice of index impacts the efficiency and accuracy of the search.\n",
    "   \n",
    "2. **Index Building:** Once you've chosen an index type, you build the index by adding your database vectors to it. This step may involve training the index if it's a learned index (e.g., IVF indices).\n",
    "\n",
    "3. **Searching:** After the index is built and populated, you can perform searches by passing query vectors to the index's search method. The method returns the IDs of the top matching vectors in the database along with their distances to the query vector.\n",
    "\n",
    "### Using HNSW in FAISS\n",
    "\n",
    "HNSW, or Hierarchical Navigable Small World graphs, is a graph-based approach that provides an excellent balance between search speed and accuracy. It's particularly effective for similarity searches in large-scale, high-dimensional datasets.\n",
    "\n",
    "Here's a basic outline of how to use HNSW in FAISS:\n",
    "\n",
    "1. **Index Creation:** First, create an HNSW index. FAISS provides the `IndexHNSWFlat` (and variants like `IndexHNSWScalarQuantizer`) for this purpose. The choice between them depends on your data and the trade-off between speed and memory usage you're willing to make.\n",
    "\n",
    "    ```python\n",
    "     import faiss\n",
    "     d = 128  # Dimension of the vectors\n",
    "     index = faiss.IndexHNSWFlat(d, M)  # M is a parameter defining the maximum number of connections for each element in the graph\n",
    "    ```\n",
    "\n",
    "2. **Index Population:** Add your vectors to the index. The vectors should be in a NumPy array.\n",
    "\n",
    "    ```python\n",
    "    xb = ...  # Your database vectors in a NumPy array\n",
    "    index.add(xb)\n",
    "    ```\n",
    "\n",
    "3. **Setting HNSW Parameters:** Before searching, you might want to adjust HNSW parameters like `efConstruction` (which affects the index building time and quality) and `efSearch` (which influences the search time and accuracy).\n",
    "\n",
    "    ```python\n",
    "    index.hnsw.efConstruction = 40\n",
    "    index.hnsw.efSearch = 16\n",
    "    ```\n",
    "\n",
    "4. **Searching:** Use the `search` method to find the top N matching vectors for your query vectors.\n",
    "\n",
    "    ```python\n",
    "    xq = ...  # Query vectors\n",
    "    k = 10  # Number of nearest neighbors to retrieve\n",
    "    D, I = index.search(xq, k)  # D are the distances, I are the indices of the nearest neighbors\n",
    "    ```\n",
    "\n",
    "### Additional Tips\n",
    "\n",
    "- **Parameter Tuning:** HNSW parameters like `M`, `efConstruction`, and `efSearch` significantly impact both the quality of the search results and the performance. Experiment with these values based on your dataset and requirements.\n",
    "- **Memory Management:** HNSW can be memory-intensive. If your dataset is very large, consider using a quantizer with HNSW or exploring other FAISS index types that offer better memory efficiency.\n",
    "- **Batching Queries:** For efficiency, FAISS supports batching queries. If you have multiple query vectors, it's faster to search for them in a batch rather than one by one.\n",
    "\n",
    "FAISS is a powerful tool, but getting the best performance out of it often requires experimentation with different index types, parameters, and optimizations specific to your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c66c3d",
   "metadata": {},
   "source": [
    "TODO:  Study HNSW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b6dd9",
   "metadata": {},
   "source": [
    "- We are fetching topK based on Distance.\n",
    "What are other ways we can fetch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529906c8",
   "metadata": {},
   "source": [
    "## KV Caching\n",
    "Implementing Key-Value (KV) caching in Retrieval-Augmented Generation (RAG) systems involves creating a caching mechanism that stores the results of retrieval operations so that future queries can access pre-fetched information without needing to perform the retrieval step again. This can significantly improve the efficiency and speed of RAG systems, especially for queries that are frequently repeated or similar to previous queries. Below is a general approach to implementing KV caching in a RAG system:\n",
    "\n",
    "### Step 1: Design the Cache Structure\n",
    "First, decide on the structure of your cache. A simple and effective structure is a key-value store, where:\n",
    "- **Key**: A representation of the query or input prompt. This could be the exact query text, a hashed version of it, or some form of embedding.\n",
    "- **Value**: The retrieved documents or information along with any metadata necessary for the generation step.\n",
    "\n",
    "### Step 2: Cache Integration Points\n",
    "Identify the points in your RAG pipeline where the cache should be checked and updated:\n",
    "- **Before Retrieval**: Check the cache using the current query as the key. If the query is found in the cache, use the cached results and skip the retrieval step.\n",
    "- **After Retrieval**: Update the cache with the new query and its retrieved documents if the query was not found in the cache initially.\n",
    "\n",
    "### Step 3: Implement Caching Logic\n",
    "Implement the caching logic within your RAG workflow. This involves:\n",
    "- **Checking the Cache**: Before performing retrieval, look up the query in the cache. If a match is found, use the cached data.\n",
    "- **Updating the Cache**: After retrieval, if the data was not in the cache, add it with the current query as the key.\n",
    "\n",
    "### Step 4: Cache Eviction and Management\n",
    "Decide on a strategy for cache eviction and management to prevent the cache from growing indefinitely. Common strategies include:\n",
    "- **Least Recently Used (LRU)**: Evict the least recently accessed items first.\n",
    "- **Time-to-Live (TTL)**: Items in the cache expire after a certain time period.\n",
    "- **Capacity**: Keep the cache size fixed, evicting items based on a predetermined strategy once the capacity is reached.\n",
    "\n",
    "### Step 5: Serialization and Persistence (Optional)\n",
    "For long-term efficiency, especially across system restarts, implement serialization and persistence for your cache. This means saving the cache to disk and loading it when the system starts.\n",
    "\n",
    "### Example Pseudocode\n",
    "```python\n",
    "class RAGCache:\n",
    "    def __init__(self, capacity=1000):\n",
    "        self.cache = LRUCache(capacity)  # Assuming an LRUCache implementation is available\n",
    "\n",
    "    def fetch_documents(self, query):\n",
    "        # Check cache\n",
    "        if query in self.cache:\n",
    "            return self.cache[query]\n",
    "        \n",
    "        # Perform retrieval\n",
    "        documents = perform_retrieval(query)\n",
    "        \n",
    "        # Update cache\n",
    "        self.cache[query] = documents\n",
    "        \n",
    "        return documents\n",
    "\n",
    "    # Additional methods for cache management, serialization, etc.\n",
    "```\n",
    "\n",
    "### Considerations\n",
    "- **Cache Key Selection**: The choice of cache key is crucial. Simple queries might use the text directly, but for more complex inputs, consider embeddings or hashed values.\n",
    "- **Cache Misses and Efficiency**: Monitor cache hit rates and adjust your caching strategy accordingly to ensure efficiency.\n",
    "- **Security and Privacy**: Be mindful of what information is being cached, especially if sensitive data is involved.\n",
    "\n",
    "Implementing KV caching in RAG systems can dramatically improve response times for repeated or similar queries, making your system more efficient and user-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16040cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Credit Risk?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff67de56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model and tokenizer from local directory: C:\\Users\\erdrr\\OneDrive\\Desktop\\Scholastic\\NLP\\LLM\\RAG\\FinsightRAG\\models\\WhereIsAI\\UAE-Large-V1\\model\n",
      "[INFO]: Generated Embedding of shape: (1, 1024)\n"
     ]
    }
   ],
   "source": [
    "embedding_vector = get_query_embeddings(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6393b1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Top 5 matching Indices: [2971, 2887, 5219, 2888, 3355, 2973, 2948, 5368, 2889, 8613]\n"
     ]
    }
   ],
   "source": [
    "indices,distance = search_embedding( embedding_vector, top_n=10,isL2Index=False)\n",
    "print(f\"[INFO]: Top {5} matching Indices: {indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9582c1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Top 5 matching Indices: [35927, 35945, 35930, 56528, 35160, 2971, 35932, 39505, 56529, 2887]\n"
     ]
    }
   ],
   "source": [
    "indices,distance = search_embedding( embedding_vector, top_n=10,isL2Index=True)\n",
    "print(f\"[INFO]: Top {5} matching Indices: {indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26aa10",
   "metadata": {},
   "source": [
    "- The top k indices can be either a mega chunk summary index or a chunk.\n",
    "- If the index is a mega chunk, we will use it as it is, other wise we will get the mega chunk for the index recived.\n",
    "\n",
    "- This will give us a list of all the mega chunks which are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af2e6eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Mega Chunks found in Top Indices: [5219, 2971, 2887, 2889, 3355, 2972]\n"
     ]
    }
   ],
   "source": [
    "mega_chunks_list = get_mega_chunks_by_indices(indices)\n",
    "print(f\"[INFO]: Mega Chunks found in Top Indices: {mega_chunks_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc6d3c",
   "metadata": {},
   "source": [
    "- Now we have a list of all the Mega Chunks\n",
    "This could be even a single chunk, if all tok_n maching indices were from a single mega chunk and its chunks.\n",
    "- Now for the list of Mega chunks, let's get all the chunk indexes which are part of this mega chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "969c7c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Total Chunks : 61\n"
     ]
    }
   ],
   "source": [
    "retrieved_data,retrieved_chunks =get_all_chunks_for_mega_chunks_list(mega_chunks_list)\n",
    "print(f\"[INFO]: Total Chunks : {len(retrieved_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581330a2",
   "metadata": {},
   "source": [
    "- We will get a maximum of `len(mega_chunks_list) * CHUNK_MULTIPLIER` chunks as mega chunks may have less chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146bb1a7",
   "metadata": {},
   "source": [
    "Now lets pass the chunks to a Reranker, and get the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a2c4a0",
   "metadata": {},
   "source": [
    "## Re-Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf30cbaa",
   "metadata": {},
   "source": [
    "Now, out of all the retrieved embeddings, lets rerank them to get best embeddings.\n",
    "we will pass all the 'chunks' (not mega chunks)\n",
    " - We need exact part of the text at the top, hence, the part of megachunk which contains the exact information will be placed at top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c50c7c",
   "metadata": {},
   "source": [
    "- Types of Reranker\n",
    "- Add some Papers\n",
    "- Explain those in brief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444cd1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mixedbread-ai/mxbai-rerank-large-v1\"\n",
    "model_save_path = Path(os.path.join(os.environ[\"MODELS_BASE_DIR\"],model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709087bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_model_is_available(model_name,model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc7c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rerank_model(model_name):\n",
    "    model_name = os.environ[\"RERANKER\"]  \n",
    "    model_save_path = Path(os.path.join(os.environ[\"MODELS_BASE_DIR\"],model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7676a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reranked_chunks(query, retrieved_chunks,return_documents=True, top_k=5):\n",
    "    model_name = os.environ[\"RERANKER\"]  \n",
    "    model_save_path = Path(os.path.join(os.environ[\"MODELS_BASE_DIR\"],model_name))\n",
    "    model = CrossEncoder(model_save_path,device=\"cpu\")\n",
    "    reranked_chunks = model.rank(query, retrieved_chunks, return_documents=True, top_k=5)\n",
    "    return reranked_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f46d1f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7891f302a7f64b19ad9bd7f95de90d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder(model_save_path,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95ae835c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9924a494604e4e668ab3edc3a35b1e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m reranked_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rag_py310\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:437\u001b[0m, in \u001b[0;36mCrossEncoder.rank\u001b[1;34m(self, query, documents, top_k, return_documents, batch_size, show_progress_bar, num_workers, activation_fct, apply_softmax, convert_to_numpy, convert_to_tensor)\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorpus_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scores[i]})\n\u001b[1;32m--> 437\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results[:top_k]\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "reranked_chunks = model.rank(query, retrieved_chunks, return_documents=True, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdcf8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84922d1d",
   "metadata": {},
   "source": [
    "# AUGMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4b93f",
   "metadata": {},
   "source": [
    "- Now we have our data ready, we need to Augment this before we can pass it to an LLM for Generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f54a7",
   "metadata": {},
   "source": [
    "- What is Augment?\n",
    "- Is it Creating Prompt?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92bd187",
   "metadata": {},
   "source": [
    "Creating a prompt to effectively leverage a local Large Language Model (LLM) for augmenting and reranking search results involves a few key steps. It should succinctly present the user's query, provide a structured format for the retrieved data, and pose the question clearly to ensure that the LLM understands what is expected in terms of augmentation and reranking. Here’s a structured approach to writing such a prompt:\n",
    "\n",
    "1. **Introduction to the Task:** Start by briefly explaining the task to the LLM.\n",
    "2. **User's Query:** Clearly state the original query from the user.\n",
    "3. **Retrieved Data:** Present the top n results fetched from the FAISS using HNSW index. It's crucial to format this data clearly, perhaps in a list or a table, with each entry containing key details that the LLM might need to understand the context or content of the result. Include relevant details such as the title, a short snippet or summary, and any metadata like a relevance score or document ID.\n",
    "4. **Question for LLM:** Specify what you need from the LLM. This could be augmenting the information with additional insights, explaining complex concepts in simpler terms, or reranking the results based on some criteria such as relevance to the query, comprehensiveness, clarity, etc.\n",
    "5. **Instructions for Output:** Provide clear instructions on how you want the output structured. This can include asking for a ranked list, suggestions for further refinement, or additional questions to ask for deeper understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14717a1c",
   "metadata": {},
   "source": [
    "### Sample Prompt:\n",
    "**Task:** You are to augment the reranked results from an investment and finance terms database with additional insights and explanations, helping to provide a deeper understanding of each entry in relation to the user's query.\n",
    "\n",
    "**User's Query:** \"What are the implications of the Federal Reserve's interest rate changes on small businesses?\"\n",
    "\n",
    "**Data:**\n",
    "\n",
    "Article 1: \"Interest rate changes can significantly affect small businesses, especially in terms of loans and credit availability. This piece dives into the specifics.\"\n",
    "Score: 93\n",
    "\n",
    "Article 2: \"The Federal Reserve adjusts interest rates to control inflation and stabilize the economy. This article explores the broad impacts of these adjustments.\"\n",
    "Score: 89\n",
    "\n",
    "Article 3: \"Examining how various economic policies, including interest rate changes, influence the well-being of small businesses.\"\n",
    "Score: 85\n",
    "\n",
    "**Instructions for Augmentation:**\n",
    "\n",
    "For each article, provide a detailed summary that extracts and highlights key points relevant to the user's query about the Federal Reserve's interest rate changes and their implications for small businesses.\n",
    "Where possible, include any actionable insights or advice derived from each article that could benefit a small business owner.\n",
    "Identify any gaps in the information provided by the articles that might require further research or clarification.\n",
    "Question for LLM: Given the reranked articles and the user's query, can you augment the provided summaries with deeper insights, emphasizing how each article addresses the query? Additionally, suggest any complementary topics or questions that the user might find useful for a comprehensive understanding of the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6930d7ae",
   "metadata": {},
   "source": [
    "**Now based on above sample prompt, lets create \"Data\" part for our prompt.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a11f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_prompt = \"\\n\\n\"\n",
    "for i,item in enumerate(reranked_chunks):\n",
    "    articles_prompt += f\"Article {i+1}: {item['text']} \\nScore:{round(item['score']*100,4)} \\n\\n\"\n",
    "print(articles_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f47a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ac5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = f\"\"\"\n",
    "Task: You are to augment the reranked results from an investment and finance terms database with additional insights and explanations, helping to provide a deeper understanding of each entry in relation to the user's query.\n",
    "\n",
    "User's Query:{query}\n",
    "\n",
    "{articles_prompt}\n",
    "Instructions for Augmentation:\n",
    "\n",
    "For each article, provide a detailed summary that extracts and highlights key points relevant to the user's query about the Federal Reserve's interest rate changes and their implications for small businesses.\n",
    "Where possible, include any actionable insights or advice derived from each article that could benefit a small business owner.\n",
    "Identify any gaps in the information provided by the articles that might require further research or clarification.\n",
    "Question for LLM: Given the reranked articles and the user's query, can you augment the provided summaries with deeper insights, emphasizing how each article addresses the query? Additionally, suggest any complementary topics or questions that the user might find useful for a comprehensive understanding of the subject.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = f\"\"\"Based on the following relevant articles items, please answer the query.\n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "Don't return the thinking, only return the answer.\n",
    "Make sure your answers are as explanatory as possible. \n",
    "\n",
    "\\nRelevant Articles: {articles_prompt}\n",
    "User query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a905ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(reranked_chunks):\n",
    "    articles_prompt = \"\\n\\n\"\n",
    "    for i,item in enumerate(reranked_chunks):\n",
    "        articles_prompt += f\"Article {i+1}: {item['text']} \\nScore:{round(item['score']*100,4)} \\n\\n\"\n",
    "        \n",
    "    base_prompt = f\"\"\"Based on the following relevant articles items, please answer the query.\n",
    "    Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "    Don't return the thinking, only return the answer.\n",
    "    Make sure your answers are as explanatory as possible. \n",
    "\n",
    "    \\nRelevant Articles: {articles_prompt}\n",
    "    User query: {query}\"\"\"\n",
    "    dialogue_template = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":base_prompt\n",
    "    }]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "710254c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reranked_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(create_prompt(\u001b[43mreranked_chunks\u001b[49m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'reranked_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "print(create_prompt(reranked_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3943c90c",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "565fda52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers.utils import is_flash_attn_2_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bf8326b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b78c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm():\n",
    "    llm_model_id = os.environ[\"LLM_MODEL\"]\n",
    "    model_save_path = Path(os.path.join(os.environ[\"MODELS_BASE_DIR\"],llm_model_id))\n",
    "    use_quantization_config = True\n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                        bnb_4but_compute_dtype=torch.float16)\n",
    "    ## Define Attention\n",
    "    if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0) >=8):\n",
    "        attn_implementation = 'flash_attention_2'\n",
    "    else:\n",
    "        attn_implementation = \"sdpa\" # Scaled Dot Procust Attention\n",
    "    print(f\"[INFO]: Using Attention: {attn_implementation}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                 torch_dtype=torch.float16,\n",
    "                                 quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                 low_cpu_mem_usage=False, # use as much memory as we can\n",
    "                                 attn_implementation=attn_implementation)#.to(get_device())\n",
    "    return llm_model,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c109f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Using Attention: sdpa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8266ee63150940d3913006ada2fae366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = get_llm(model_save_path,use_quantization_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e651c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a904c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d51cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tokens(prompt,model):\n",
    "# Tokenize the input Text (Prompt) - Turn it number nad send to GPU\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    input_ids = tokenizer(prompt,\n",
    "                          return_tensors='pt')#.to(\"cpu\")\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_response(input_ids,llm_model,tokenizer, max_new_tokens=512):\n",
    "    ## Generate Outputs from Local LLM\n",
    "    outputs = llm_model.generate(**input_ids,\n",
    "                            max_new_tokens = 256)\n",
    "    outputs_decoded = tokenizer.decode(outputs[0])\n",
    "    return outputs_decoded[outputs_decoded.find(\"<start_of_turn>model\"):].strip(\"<start_of_turn>model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db8bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44679c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945ce0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ba2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model Output Decodes: \\n{outputs_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c83cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd070973",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs_decoded[outputs_decoded.find(\"<start_of_turn>model\"):].strip(\"<start_of_turn>model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8dc74f",
   "metadata": {},
   "source": [
    "# Lets RAG!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc87ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(reranked_chunks):\n",
    "    \"\"\"\n",
    "    Augmentation: Augment the Retrived data and create prompt.\n",
    "    \"\"\"\n",
    "    articles_prompt = \"\\n\"\n",
    "    for i,item in enumerate(reranked_chunks):\n",
    "        articles_prompt += f\"Article {i+1}: {item['text']} \\nScore:{round(item['score']*100,4)} \\n\\n\"\n",
    "        \n",
    "    base_prompt = f\"\"\"Answer the following question in a concise and informative manner:\n",
    "    \n",
    "{query}\n",
    "    \n",
    "Use the below Articles to answer the above question based on their relevance score. Create a combined answer from the articles:\n",
    "{articles_prompt}\n",
    "\"\"\"\n",
    "    dialogue_template = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":base_prompt\n",
    "    }]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d92d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is a record of revenue or expenses that have been earned or incurred but have not yet been recorded in the company's financial statements Called?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11c6da40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model and tokenizer from local directory: C:\\Users\\erdrr\\OneDrive\\Desktop\\Scholastic\\NLP\\LLM\\RAG\\FinsightRAG\\models\\WhereIsAI\\UAE-Large-V1\\model\n",
      "[INFO]: Generated Embedding of shape: (1, 1024)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b74b8466434b858a36c054dbafe11b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m mega_chunks_list \u001b[38;5;241m=\u001b[39m get_mega_chunks_by_indices(indices)\n\u001b[0;32m      4\u001b[0m retrieved_data,retrieved_chunks \u001b[38;5;241m=\u001b[39mget_all_chunks_for_mega_chunks_list(mega_chunks_list)\n\u001b[1;32m----> 5\u001b[0m reranked_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rag_py310\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:437\u001b[0m, in \u001b[0;36mCrossEncoder.rank\u001b[1;34m(self, query, documents, top_k, return_documents, batch_size, show_progress_bar, num_workers, activation_fct, apply_softmax, convert_to_numpy, convert_to_tensor)\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorpus_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scores[i]})\n\u001b[1;32m--> 437\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results[:top_k]\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "embedding_vector = get_query_embeddings(query)\n",
    "indices,distance = search_embedding(embedding_vector, top_n=10,isL2Index=True)\n",
    "mega_chunks_list = get_mega_chunks_by_indices(indices)\n",
    "retrieved_data,retrieved_chunks =get_all_chunks_for_mega_chunks_list(mega_chunks_list)\n",
    "reranked_chunks = model.rank(query, retrieved_chunks, return_documents=True, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_query = create_prompt(reranked_chunks)\n",
    "#print(augmented_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f3a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6e28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fcc6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2948977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce760945",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Generate Outputs from Local LLM\n",
    "input_ids = tokenizer(augmented_query,\n",
    "                      return_tensors='pt').to(\"cuda\")\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                            max_new_tokens = 1024)\n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(outputs_decoded[outputs_decoded.find(\"<start_of_turn>model\"):].strip(\"<start_of_turn>model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a7e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_py310",
   "language": "python",
   "name": "rag_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
